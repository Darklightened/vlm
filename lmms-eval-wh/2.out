The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: woohyeon (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-eval-wh/wandb/run-20241230_114306-lfutj3a4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/lfutj3a4/workspace
[32m2024-12-30 11:43:08.741[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-30 11:43:10.448[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-30 11:43:11.337[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-30 11:43:11.339[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-30 11:43:11.342[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
initialize llava model with modification
OpenCLIP not installed
self.merging= None
model_name: llava-v1.6-vicuna-7b
Loaded LLaVA model: liuhaotian/llava-v1.6-vicuna-7b
loding from here
Loading vision tower: openai/clip-vit-large-patch14-336
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.60s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:05,  5.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.22s/it]
Model Class: LlavaLlamaForCausalLM
device: cuda:2
generation_type: recursion
fix_grid: 2x2
attention_thresholding_type: layer_mean_topk
attention_norm: None
attention_threshold: 0.9
detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [-2, -1, 0, 1]
positional_embedding_type: reduced
visualize_heatmap: False
square: 2
remove unpadding=True, change to 'spatial'
change positional embedding to reduced
Reduced embedding type.
Reduced embedding type.
[32m2024-12-30 11:43:35.301[0m | [1mINFO    [0m | [36mlmms_eval.models.llava[0m:[36m__init__[0m:[36m321[0m - [1mUsing single device: cuda:2[0m
[32m2024-12-30 11:43:35.531[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-30 11:43:35.532[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2024-12-30 11:43:35.533[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 0...[0m
  0%|          | 0/500 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 112453.86it/s]
[32m2024-12-30 11:43:36.958[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 500[0m
[32m2024-12-30 11:43:36.959[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/500 [00:00<?, ?it/s]CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
Model Responding:   0%|          | 1/500 [00:04<34:06,  4.10s/it]Model Responding:   0%|          | 2/500 [00:06<27:18,  3.29s/it]Model Responding:   1%|          | 3/500 [00:09<24:02,  2.90s/it]Model Responding:   1%|          | 4/500 [00:11<22:04,  2.67s/it]Model Responding:   1%|          | 5/500 [00:14<21:23,  2.59s/it]Model Responding:   1%|          | 6/500 [00:16<20:53,  2.54s/it]Model Responding:   1%|‚ñè         | 7/500 [00:18<20:27,  2.49s/it]Model Responding:   2%|‚ñè         | 8/500 [00:21<20:29,  2.50s/it]Model Responding:   2%|‚ñè         | 9/500 [00:23<20:43,  2.53s/it]Model Responding:   2%|‚ñè         | 10/500 [00:26<20:36,  2.52s/it]Model Responding:   2%|‚ñè         | 11/500 [00:29<20:37,  2.53s/it]Model Responding:   2%|‚ñè         | 12/500 [00:31<20:33,  2.53s/it]Model Responding:   3%|‚ñé         | 13/500 [00:34<20:31,  2.53s/it]Model Responding:   3%|‚ñé         | 14/500 [00:36<19:53,  2.46s/it]Model Responding:   3%|‚ñé         | 15/500 [00:38<19:27,  2.41s/it]Model Responding:   3%|‚ñé         | 16/500 [00:40<19:10,  2.38s/it]Model Responding:   3%|‚ñé         | 17/500 [00:43<19:22,  2.41s/it]Model Responding:   4%|‚ñé         | 18/500 [00:45<19:28,  2.42s/it]Model Responding:   4%|‚ñç         | 19/500 [00:48<19:22,  2.42s/it]Model Responding:   4%|‚ñç         | 20/500 [00:50<19:20,  2.42s/it]Model Responding:   4%|‚ñç         | 21/500 [00:53<20:02,  2.51s/it]Model Responding:   4%|‚ñç         | 22/500 [00:55<19:31,  2.45s/it]Model Responding:   5%|‚ñç         | 23/500 [00:58<19:03,  2.40s/it]Model Responding:   5%|‚ñç         | 24/500 [01:00<19:13,  2.42s/it]Model Responding:   5%|‚ñå         | 25/500 [01:03<19:26,  2.46s/it]Model Responding:   5%|‚ñå         | 26/500 [01:05<19:20,  2.45s/it]Model Responding:   5%|‚ñå         | 27/500 [01:07<18:58,  2.41s/it]Model Responding:   6%|‚ñå         | 28/500 [01:10<19:16,  2.45s/it]Model Responding:   6%|‚ñå         | 29/500 [01:12<18:51,  2.40s/it]Model Responding:   6%|‚ñå         | 30/500 [01:15<19:09,  2.45s/it]Model Responding:   6%|‚ñå         | 31/500 [01:17<18:42,  2.39s/it]Model Responding:   6%|‚ñã         | 32/500 [01:19<18:24,  2.36s/it]Model Responding:   7%|‚ñã         | 33/500 [01:22<18:20,  2.36s/it]Model Responding:   7%|‚ñã         | 34/500 [01:24<18:11,  2.34s/it]Model Responding:   7%|‚ñã         | 35/500 [01:26<18:27,  2.38s/it]Model Responding:   7%|‚ñã         | 36/500 [01:29<18:27,  2.39s/it]Model Responding:   7%|‚ñã         | 37/500 [01:31<18:16,  2.37s/it]Model Responding:   8%|‚ñä         | 38/500 [01:34<18:24,  2.39s/it]Model Responding:   8%|‚ñä         | 39/500 [01:36<18:33,  2.42s/it]Model Responding:   8%|‚ñä         | 40/500 [01:39<19:54,  2.60s/it]Model Responding:   8%|‚ñä         | 41/500 [01:41<19:16,  2.52s/it]Model Responding:   8%|‚ñä         | 42/500 [01:44<19:10,  2.51s/it]Model Responding:   9%|‚ñä         | 43/500 [01:46<18:44,  2.46s/it]Model Responding:   9%|‚ñâ         | 44/500 [01:49<18:48,  2.47s/it]Model Responding:   9%|‚ñâ         | 45/500 [01:51<18:46,  2.48s/it]Model Responding:   9%|‚ñâ         | 46/500 [01:54<18:27,  2.44s/it]Model Responding:   9%|‚ñâ         | 47/500 [01:56<18:20,  2.43s/it]Model Responding:  10%|‚ñâ         | 48/500 [01:58<18:07,  2.41s/it]Model Responding:  10%|‚ñâ         | 49/500 [02:01<18:34,  2.47s/it]Model Responding:  10%|‚ñà         | 50/500 [02:03<18:30,  2.47s/it]Model Responding:  10%|‚ñà         | 51/500 [02:06<18:25,  2.46s/it]Model Responding:  10%|‚ñà         | 52/500 [02:08<17:56,  2.40s/it]Model Responding:  11%|‚ñà         | 53/500 [02:10<17:44,  2.38s/it]Model Responding:  11%|‚ñà         | 54/500 [02:13<17:49,  2.40s/it]Model Responding:  11%|‚ñà         | 55/500 [02:16<18:26,  2.49s/it]Model Responding:  11%|‚ñà         | 56/500 [02:18<18:08,  2.45s/it]Model Responding:  11%|‚ñà‚ñè        | 57/500 [02:20<18:03,  2.45s/it]Model Responding:  12%|‚ñà‚ñè        | 58/500 [02:23<18:05,  2.46s/it]Model Responding:  12%|‚ñà‚ñè        | 59/500 [02:25<17:57,  2.44s/it]Model Responding:  12%|‚ñà‚ñè        | 60/500 [02:28<17:47,  2.43s/it]Model Responding:  12%|‚ñà‚ñè        | 61/500 [02:30<17:44,  2.42s/it]Model Responding:  12%|‚ñà‚ñè        | 62/500 [02:32<17:28,  2.39s/it]Model Responding:  13%|‚ñà‚ñé        | 63/500 [02:35<17:14,  2.37s/it]Model Responding:  13%|‚ñà‚ñé        | 64/500 [02:37<16:59,  2.34s/it]Model Responding:  13%|‚ñà‚ñé        | 65/500 [02:39<16:47,  2.32s/it]Model Responding:  13%|‚ñà‚ñé        | 66/500 [02:42<16:53,  2.34s/it]Model Responding:  13%|‚ñà‚ñé        | 67/500 [02:44<17:17,  2.40s/it]Model Responding:  14%|‚ñà‚ñé        | 68/500 [02:47<17:19,  2.41s/it]Model Responding:  14%|‚ñà‚ñç        | 69/500 [02:49<17:36,  2.45s/it]Model Responding:  14%|‚ñà‚ñç        | 70/500 [02:52<18:00,  2.51s/it]Model Responding:  14%|‚ñà‚ñç        | 71/500 [02:54<17:56,  2.51s/it]Model Responding:  14%|‚ñà‚ñç        | 72/500 [02:57<17:27,  2.45s/it]Model Responding:  15%|‚ñà‚ñç        | 73/500 [02:59<17:26,  2.45s/it]Model Responding:  15%|‚ñà‚ñç        | 74/500 [03:02<17:38,  2.48s/it]Model Responding:  15%|‚ñà‚ñå        | 75/500 [03:04<17:37,  2.49s/it]Model Responding:  15%|‚ñà‚ñå        | 76/500 [03:06<17:17,  2.45s/it]Model Responding:  15%|‚ñà‚ñå        | 77/500 [03:09<16:59,  2.41s/it]Model Responding:  16%|‚ñà‚ñå        | 78/500 [03:11<16:32,  2.35s/it]Model Responding:  16%|‚ñà‚ñå        | 79/500 [03:13<16:22,  2.33s/it]Model Responding:  16%|‚ñà‚ñå        | 80/500 [03:16<16:20,  2.33s/it]Model Responding:  16%|‚ñà‚ñå        | 81/500 [03:18<16:02,  2.30s/it]Model Responding:  16%|‚ñà‚ñã        | 82/500 [03:20<16:14,  2.33s/it]Model Responding:  17%|‚ñà‚ñã        | 83/500 [03:23<16:12,  2.33s/it]Model Responding:  17%|‚ñà‚ñã        | 84/500 [03:25<15:59,  2.31s/it]Model Responding:  17%|‚ñà‚ñã        | 85/500 [03:27<16:01,  2.32s/it]Model Responding:  17%|‚ñà‚ñã        | 86/500 [03:29<15:53,  2.30s/it]Model Responding:  17%|‚ñà‚ñã        | 87/500 [03:32<15:57,  2.32s/it]Model Responding:  18%|‚ñà‚ñä        | 88/500 [03:34<15:51,  2.31s/it]Model Responding:  18%|‚ñà‚ñä        | 89/500 [03:36<15:46,  2.30s/it]Model Responding:  18%|‚ñà‚ñä        | 90/500 [03:39<15:41,  2.30s/it]Model Responding:  18%|‚ñà‚ñä        | 91/500 [03:41<15:45,  2.31s/it]Model Responding:  18%|‚ñà‚ñä        | 92/500 [03:43<16:00,  2.35s/it]Model Responding:  19%|‚ñà‚ñä        | 93/500 [03:46<15:45,  2.32s/it]Model Responding:  19%|‚ñà‚ñâ        | 94/500 [03:48<15:44,  2.33s/it]Model Responding:  19%|‚ñà‚ñâ        | 95/500 [03:50<15:52,  2.35s/it]Model Responding:  19%|‚ñà‚ñâ        | 96/500 [03:53<15:45,  2.34s/it]Model Responding:  19%|‚ñà‚ñâ        | 97/500 [03:55<15:25,  2.30s/it]Model Responding:  20%|‚ñà‚ñâ        | 98/500 [03:57<15:14,  2.28s/it]