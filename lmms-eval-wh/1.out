The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: woohyeon (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-eval-wh/wandb/run-20241230_114157-oyollzmw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run red-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/oyollzmw/workspace
[32m2024-12-30 11:41:59.631[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-30 11:42:01.624[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-30 11:42:02.381[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-30 11:42:02.382[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-30 11:42:02.385[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
initialize llava model with modification
OpenCLIP not installed
self.merging= None
model_name: llava-v1.6-vicuna-7b
Loaded LLaVA model: liuhaotian/llava-v1.6-vicuna-7b
loding from here
Loading vision tower: openai/clip-vit-large-patch14-336
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.71s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:05,  5.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  4.82s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.01s/it]
Model Class: LlavaLlamaForCausalLM
device: cuda:3
generation_type: recursion
fix_grid: 2x2
attention_thresholding_type: layer_mean_topk
attention_norm: None
attention_threshold: 0.9
detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [-2, -1, 0, 1]
positional_embedding_type: reduced
visualize_heatmap: False
square: 1
remove unpadding=True, change to 'spatial'
change positional embedding to reduced
Reduced embedding type.
Reduced embedding type.
[32m2024-12-30 11:42:27.329[0m | [1mINFO    [0m | [36mlmms_eval.models.llava[0m:[36m__init__[0m:[36m321[0m - [1mUsing single device: cuda:3[0m
[32m2024-12-30 11:42:27.586[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-30 11:42:27.587[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2024-12-30 11:42:27.587[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 0...[0m
  0%|          | 0/500 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 117553.36it/s]
[32m2024-12-30 11:42:28.806[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 500[0m
[32m2024-12-30 11:42:28.806[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/500 [00:00<?, ?it/s]CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
Model Responding:   0%|          | 1/500 [00:03<32:56,  3.96s/it]Model Responding:   0%|          | 2/500 [00:06<26:30,  3.19s/it]Model Responding:   1%|          | 3/500 [00:09<24:07,  2.91s/it]Model Responding:   1%|          | 4/500 [00:11<22:47,  2.76s/it]Model Responding:   1%|          | 5/500 [00:14<22:02,  2.67s/it]Model Responding:   1%|          | 6/500 [00:16<21:12,  2.57s/it]Model Responding:   1%|‚ñè         | 7/500 [00:19<20:57,  2.55s/it]Model Responding:   2%|‚ñè         | 8/500 [00:21<21:18,  2.60s/it]Model Responding:   2%|‚ñè         | 9/500 [00:24<21:19,  2.61s/it]Model Responding:   2%|‚ñè         | 10/500 [00:26<20:49,  2.55s/it]Model Responding:   2%|‚ñè         | 11/500 [00:29<21:06,  2.59s/it]Model Responding:   2%|‚ñè         | 12/500 [00:32<21:17,  2.62s/it]Model Responding:   3%|‚ñé         | 13/500 [00:34<21:21,  2.63s/it]Model Responding:   3%|‚ñé         | 14/500 [00:37<21:03,  2.60s/it]Model Responding:   3%|‚ñé         | 15/500 [00:40<21:08,  2.61s/it]Model Responding:   3%|‚ñé         | 16/500 [00:42<21:10,  2.62s/it]Model Responding:   3%|‚ñé         | 17/500 [00:45<20:42,  2.57s/it]Model Responding:   4%|‚ñé         | 18/500 [00:47<20:20,  2.53s/it]Model Responding:   4%|‚ñç         | 19/500 [00:50<20:38,  2.58s/it]Model Responding:   4%|‚ñç         | 20/500 [00:53<20:57,  2.62s/it]Model Responding:   4%|‚ñç         | 21/500 [00:55<21:17,  2.67s/it]Model Responding:   4%|‚ñç         | 22/500 [00:58<20:53,  2.62s/it]Model Responding:   5%|‚ñç         | 23/500 [01:01<21:06,  2.65s/it]Model Responding:   5%|‚ñç         | 24/500 [01:03<20:52,  2.63s/it]Model Responding:   5%|‚ñå         | 25/500 [01:06<20:40,  2.61s/it]Model Responding:   5%|‚ñå         | 26/500 [01:08<20:39,  2.61s/it]Model Responding:   5%|‚ñå         | 27/500 [01:11<20:31,  2.60s/it]Model Responding:   6%|‚ñå         | 28/500 [01:14<20:56,  2.66s/it]Model Responding:   6%|‚ñå         | 29/500 [01:16<20:29,  2.61s/it]Model Responding:   6%|‚ñå         | 30/500 [01:19<20:38,  2.63s/it]Model Responding:   6%|‚ñå         | 31/500 [01:21<20:26,  2.61s/it]Model Responding:   6%|‚ñã         | 32/500 [01:24<20:02,  2.57s/it]Model Responding:   7%|‚ñã         | 33/500 [01:26<19:36,  2.52s/it]Model Responding:   7%|‚ñã         | 34/500 [01:29<19:31,  2.51s/it]Model Responding:   7%|‚ñã         | 35/500 [01:31<19:39,  2.54s/it]Model Responding:   7%|‚ñã         | 36/500 [01:34<19:45,  2.56s/it]Model Responding:   7%|‚ñã         | 37/500 [01:37<19:44,  2.56s/it]Model Responding:   8%|‚ñä         | 38/500 [01:39<20:08,  2.62s/it]Model Responding:   8%|‚ñä         | 39/500 [01:42<19:42,  2.56s/it]Model Responding:   8%|‚ñä         | 40/500 [01:45<21:10,  2.76s/it]Model Responding:   8%|‚ñä         | 41/500 [01:47<20:27,  2.68s/it]Model Responding:   8%|‚ñä         | 42/500 [01:50<20:13,  2.65s/it]Model Responding:   9%|‚ñä         | 43/500 [01:53<20:17,  2.66s/it]Model Responding:   9%|‚ñâ         | 44/500 [01:55<19:57,  2.63s/it]Model Responding:   9%|‚ñâ         | 45/500 [01:58<19:50,  2.62s/it]Model Responding:   9%|‚ñâ         | 46/500 [02:00<19:46,  2.61s/it]Model Responding:   9%|‚ñâ         | 47/500 [02:03<19:37,  2.60s/it]Model Responding:  10%|‚ñâ         | 48/500 [02:05<19:16,  2.56s/it]Model Responding:  10%|‚ñâ         | 49/500 [02:08<19:55,  2.65s/it]Model Responding:  10%|‚ñà         | 50/500 [02:11<19:53,  2.65s/it]Model Responding:  10%|‚ñà         | 51/500 [02:14<19:46,  2.64s/it]Model Responding:  10%|‚ñà         | 52/500 [02:16<19:25,  2.60s/it]Model Responding:  11%|‚ñà         | 53/500 [02:19<19:14,  2.58s/it]Model Responding:  11%|‚ñà         | 54/500 [02:21<19:23,  2.61s/it]Model Responding:  11%|‚ñà         | 55/500 [02:24<19:29,  2.63s/it]Model Responding:  11%|‚ñà         | 56/500 [02:27<19:20,  2.61s/it]Model Responding:  11%|‚ñà‚ñè        | 57/500 [02:29<19:17,  2.61s/it]Model Responding:  12%|‚ñà‚ñè        | 58/500 [02:32<19:34,  2.66s/it]Model Responding:  12%|‚ñà‚ñè        | 59/500 [02:35<19:19,  2.63s/it]Model Responding:  12%|‚ñà‚ñè        | 60/500 [02:37<19:00,  2.59s/it]Model Responding:  12%|‚ñà‚ñè        | 61/500 [02:40<18:50,  2.58s/it]Model Responding:  12%|‚ñà‚ñè        | 62/500 [02:42<18:58,  2.60s/it]Model Responding:  13%|‚ñà‚ñé        | 63/500 [02:45<18:41,  2.57s/it]Model Responding:  13%|‚ñà‚ñé        | 64/500 [02:47<18:40,  2.57s/it]Model Responding:  13%|‚ñà‚ñé        | 65/500 [02:50<19:17,  2.66s/it]Model Responding:  13%|‚ñà‚ñé        | 66/500 [02:53<19:31,  2.70s/it]Model Responding:  13%|‚ñà‚ñé        | 67/500 [02:56<19:30,  2.70s/it]Model Responding:  14%|‚ñà‚ñé        | 68/500 [02:58<19:36,  2.72s/it]Model Responding:  14%|‚ñà‚ñç        | 69/500 [03:01<19:34,  2.72s/it]Model Responding:  14%|‚ñà‚ñç        | 70/500 [03:04<19:51,  2.77s/it]Model Responding:  14%|‚ñà‚ñç        | 71/500 [03:07<20:06,  2.81s/it]Model Responding:  14%|‚ñà‚ñç        | 72/500 [03:10<20:01,  2.81s/it]Model Responding:  15%|‚ñà‚ñç        | 73/500 [03:13<20:15,  2.85s/it]Model Responding:  15%|‚ñà‚ñç        | 74/500 [03:15<19:56,  2.81s/it]Model Responding:  15%|‚ñà‚ñå        | 75/500 [03:18<20:01,  2.83s/it]Model Responding:  15%|‚ñà‚ñå        | 76/500 [03:21<20:12,  2.86s/it]Model Responding:  15%|‚ñà‚ñå        | 77/500 [03:24<20:21,  2.89s/it]Model Responding:  16%|‚ñà‚ñå        | 78/500 [03:27<19:53,  2.83s/it]Model Responding:  16%|‚ñà‚ñå        | 79/500 [03:30<19:36,  2.79s/it]Model Responding:  16%|‚ñà‚ñå        | 80/500 [03:32<18:57,  2.71s/it]Model Responding:  16%|‚ñà‚ñå        | 81/500 [03:35<18:59,  2.72s/it]Model Responding:  16%|‚ñà‚ñã        | 82/500 [03:37<18:29,  2.65s/it]Model Responding:  17%|‚ñà‚ñã        | 83/500 [03:40<19:00,  2.74s/it]Model Responding:  17%|‚ñà‚ñã        | 84/500 [03:42<17:42,  2.55s/it]Model Responding:  17%|‚ñà‚ñã        | 85/500 [03:45<18:03,  2.61s/it]Model Responding:  17%|‚ñà‚ñã        | 86/500 [03:47<16:46,  2.43s/it]Model Responding:  17%|‚ñà‚ñã        | 87/500 [03:50<17:28,  2.54s/it]Model Responding:  18%|‚ñà‚ñä        | 88/500 [03:52<17:21,  2.53s/it]Model Responding:  18%|‚ñà‚ñä        | 89/500 [03:55<17:35,  2.57s/it]Model Responding:  18%|‚ñà‚ñä        | 90/500 [03:58<17:13,  2.52s/it]Model Responding:  18%|‚ñà‚ñä        | 91/500 [04:00<17:07,  2.51s/it]Model Responding:  18%|‚ñà‚ñä        | 92/500 [04:02<16:59,  2.50s/it]Model Responding:  19%|‚ñà‚ñä        | 93/500 [04:05<17:20,  2.56s/it]Model Responding:  19%|‚ñà‚ñâ        | 94/500 [04:07<16:49,  2.49s/it]Model Responding:  19%|‚ñà‚ñâ        | 95/500 [04:10<17:50,  2.64s/it]Model Responding:  19%|‚ñà‚ñâ        | 96/500 [04:13<16:52,  2.51s/it]Model Responding:  19%|‚ñà‚ñâ        | 97/500 [04:15<16:44,  2.49s/it]Model Responding:  20%|‚ñà‚ñâ        | 98/500 [04:18<17:09,  2.56s/it]Model Responding:  20%|‚ñà‚ñâ        | 99/500 [04:21<17:27,  2.61s/it]Model Responding:  20%|‚ñà‚ñà        | 100/500 [04:23<16:58,  2.55s/it]Model Responding:  20%|‚ñà‚ñà        | 101/500 [04:25<16:32,  2.49s/it]Model Responding:  20%|‚ñà‚ñà        | 102/500 [04:28<16:11,  2.44s/it]Model Responding:  21%|‚ñà‚ñà        | 103/500 [04:31<17:22,  2.63s/it]Model Responding:  21%|‚ñà‚ñà        | 104/500 [04:33<17:36,  2.67s/it]Model Responding:  21%|‚ñà‚ñà        | 105/500 [04:36<17:45,  2.70s/it]Model Responding:  21%|‚ñà‚ñà        | 106/500 [04:39<17:58,  2.74s/it]Model Responding:  21%|‚ñà‚ñà‚ñè       | 107/500 [04:42<18:06,  2.77s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 108/500 [04:44<17:22,  2.66s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 109/500 [04:47<17:36,  2.70s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 110/500 [04:50<17:15,  2.66s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 111/500 [04:52<17:15,  2.66s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 112/500 [04:55<17:01,  2.63s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 113/500 [04:57<16:43,  2.59s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 114/500 [05:00<16:56,  2.63s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 115/500 [05:03<16:53,  2.63s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 116/500 [05:05<16:35,  2.59s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 117/500 [05:08<16:24,  2.57s/it]