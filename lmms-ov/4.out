The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: woohyeon (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-ov/wandb/run-20250125_184039-at0xyya8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vqa_grid_exp1.0_attn_run_0_params_contrastive_0.20_0.10_0.10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_ov_0.5_cd
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_ov_0.5_cd/runs/at0xyya8/workspace
[32m2025-01-25 18:40:42.492[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m337[0m - [1mVerbosity set to DEBUG[0m
[32m2025-01-25 18:40:43.806[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2025-01-25 18:40:44.000[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m478[0m - [34m[1mFile illusionvqa.yaml in /workspace/vlm/lmms-ov/lmms_eval/tasks/illusionvqa could not be loaded as a task or group[0m
[32m2025-01-25 18:40:46.020[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m420[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2025-01-25 18:40:46.021[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m509[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2025-01-25 18:40:46.024[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
initialize llava model with modification
OpenCLIP not installed
LlavaRecursionConfig(stages=(-2, -1, 0, 1), positional_embedding_type='bilinear_interpolation', generation_type='recursion', attention_thresholding_type='attn_topk', attn_norm=None, attention_threshold=[1.0, 1.0, 1.0], save_output=False, output_csv_path='generation_output.csv', output_json_path='generation_output.json', contrastive_alphas=[0.2, 0.1, 0.1], square=1, _device='cuda:0', use_noised_for_contrastive=False, cd_strategy='default')
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-0.5b-ov
loading here.
Overwriting config with {'mm_spatial_pool_stride': 2, 'mm_spatial_pool_mode': 'bilinear'}
Loading vision tower: google/siglip-so400m-patch14-384
Loading vision tower: google/siglip-so400m-patch14-384
change positional embedding to bilinear_interpolation
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
Downsampled towers initialized.
Model Class: LlavaQwenForRecursion
[32m2025-01-25 18:40:59.082[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2025-01-25 18:40:59.082[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2025-01-25 18:40:59.083[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 0...[0m
  0%|          | 0/500 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 12589.69it/s]
[32m2025-01-25 18:40:59.124[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m441[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 500[0m
[32m2025-01-25 18:40:59.125[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m462[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/500 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
Model Responding:   0%|          | 1/500 [00:03<26:17,  3.16s/it]Model Responding:   0%|          | 2/500 [00:06<27:25,  3.30s/it]Model Responding:   1%|          | 3/500 [00:09<25:39,  3.10s/it]Model Responding:   1%|          | 4/500 [00:12<25:42,  3.11s/it]Model Responding:   1%|          | 5/500 [00:15<25:35,  3.10s/it]Model Responding:   1%|          | 6/500 [00:17<22:15,  2.70s/it]Model Responding:   1%|‚ñè         | 7/500 [00:19<19:38,  2.39s/it]Model Responding:   2%|‚ñè         | 8/500 [00:22<21:23,  2.61s/it]Model Responding:   2%|‚ñè         | 9/500 [00:26<25:45,  3.15s/it]Model Responding:   2%|‚ñè         | 10/500 [00:28<22:19,  2.73s/it]Model Responding:   2%|‚ñè         | 11/500 [00:31<23:05,  2.83s/it]Model Responding:   2%|‚ñè         | 12/500 [00:33<21:38,  2.66s/it]Model Responding:   3%|‚ñé         | 13/500 [00:37<23:41,  2.92s/it]Model Responding:   3%|‚ñé         | 14/500 [00:39<21:14,  2.62s/it]Model Responding:   3%|‚ñé         | 15/500 [00:43<25:39,  3.17s/it]Model Responding:   3%|‚ñé         | 16/500 [00:45<23:19,  2.89s/it]Model Responding:   3%|‚ñé         | 17/500 [00:47<20:53,  2.59s/it]Model Responding:   4%|‚ñé         | 18/500 [00:49<18:54,  2.35s/it]Model Responding:   4%|‚ñç         | 19/500 [00:51<18:04,  2.25s/it]Model Responding:   4%|‚ñç         | 20/500 [00:57<27:30,  3.44s/it]Model Responding:   4%|‚ñç         | 21/500 [00:59<23:53,  2.99s/it]Model Responding:   4%|‚ñç         | 22/500 [01:03<24:44,  3.11s/it]Model Responding:   5%|‚ñç         | 23/500 [01:07<27:34,  3.47s/it]Model Responding:   5%|‚ñç         | 24/500 [01:10<26:23,  3.33s/it]Model Responding:   5%|‚ñå         | 25/500 [01:16<31:50,  4.02s/it]Model Responding:   5%|‚ñå         | 26/500 [01:18<26:44,  3.38s/it]Model Responding:   5%|‚ñå         | 27/500 [01:20<23:33,  2.99s/it]Model Responding:   6%|‚ñå         | 28/500 [01:22<21:07,  2.69s/it]Model Responding:   6%|‚ñå         | 29/500 [01:25<21:57,  2.80s/it]Model Responding:   6%|‚ñå         | 30/500 [01:27<19:56,  2.54s/it]Model Responding:   6%|‚ñå         | 31/500 [01:29<19:43,  2.52s/it]Model Responding:   6%|‚ñã         | 32/500 [01:31<19:12,  2.46s/it]Model Responding:   7%|‚ñã         | 33/500 [01:34<18:40,  2.40s/it]Model Responding:   7%|‚ñã         | 34/500 [01:36<17:25,  2.24s/it]Model Responding:   7%|‚ñã         | 35/500 [01:38<17:18,  2.23s/it]Model Responding:   7%|‚ñã         | 36/500 [01:40<17:55,  2.32s/it]Model Responding:   7%|‚ñã         | 37/500 [01:45<22:25,  2.91s/it]Model Responding:   8%|‚ñä         | 38/500 [01:47<20:12,  2.62s/it]Model Responding:   8%|‚ñä         | 39/500 [01:49<19:04,  2.48s/it]Model Responding:   8%|‚ñä         | 40/500 [01:51<18:41,  2.44s/it]Model Responding:   8%|‚ñä         | 41/500 [01:55<22:43,  2.97s/it]Model Responding:   8%|‚ñä         | 42/500 [01:57<19:59,  2.62s/it]Model Responding:   9%|‚ñä         | 43/500 [01:59<18:56,  2.49s/it]