The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: woohyeon (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-ov/wandb/run-20250125_184237-4d4sr201
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mmstar_grid_exp1.0_attn_run_0_params_contrastive_0.30_0.10_0.10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_ov_0.5_cd
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_ov_0.5_cd/runs/4d4sr201/workspace
[32m2025-01-25 18:42:40.195[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m337[0m - [1mVerbosity set to DEBUG[0m
[32m2025-01-25 18:42:41.890[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2025-01-25 18:42:42.074[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m478[0m - [34m[1mFile illusionvqa.yaml in /workspace/vlm/lmms-ov/lmms_eval/tasks/illusionvqa could not be loaded as a task or group[0m
[32m2025-01-25 18:42:43.983[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m420[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2025-01-25 18:42:43.984[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m509[0m - [1mSelected Tasks: ['mmstar'][0m
[32m2025-01-25 18:42:43.987[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
initialize llava model with modification
OpenCLIP not installed
LlavaRecursionConfig(stages=(-2, -1, 0, 1), positional_embedding_type='bilinear_interpolation', generation_type='recursion', attention_thresholding_type='attn_topk', attn_norm=None, attention_threshold=[1.0, 1.0, 1.0], save_output=False, output_csv_path='generation_output.csv', output_json_path='generation_output.json', contrastive_alphas=[0.3, 0.1, 0.1], square=1, _device='cuda:0', use_noised_for_contrastive=False, cd_strategy='default')
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-0.5b-ov
loading here.
Overwriting config with {'mm_spatial_pool_stride': 2, 'mm_spatial_pool_mode': 'bilinear'}
Loading vision tower: google/siglip-so400m-patch14-384
Loading vision tower: google/siglip-so400m-patch14-384
change positional embedding to bilinear_interpolation
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
Downsampled towers initialized.
Model Class: LlavaQwenForRecursion
[32m2025-01-25 18:42:55.756[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for mmstar, using default n_shot=0[0m
[32m2025-01-25 18:42:55.756[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-mmstar-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2025-01-25 18:42:55.758[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for mmstar on rank 0...[0m
  0%|          | 0/1500 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [00:00<00:00, 20161.30it/s]
[32m2025-01-25 18:42:55.835[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m441[0m - [34m[1mTask: mmstar; number of requests on this rank: 1500[0m
[32m2025-01-25 18:42:55.836[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m462[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/1500 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
Model Responding:   0%|          | 1/1500 [00:03<1:26:54,  3.48s/it]