The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: wjk9904 (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wjk9904 (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wjk9904 (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wjk9904 (VLM_Hallucination_Woohyeon). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-reverse/wandb/run-20241216_104753-izm7shys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reverse_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/izm7shys/workspace
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-reverse/wandb/run-20241216_104753-6te73gty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reverse_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/6te73gty/workspace
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-reverse/wandb/run-20241216_104753-4miikfxa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reverse_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/4miikfxa/workspace
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /workspace/vlm/lmms-reverse/wandb/run-20241216_104753-uduei75j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reverse_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126
wandb: üöÄ View run at https://wandb.ai/VLM_Hallucination_Woohyeon/llava1.6_recursive_eval_1126/runs/uduei75j/workspace
[32m2024-12-16 10:47:56.049[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-16 10:47:56.059[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-16 10:47:56.083[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-16 10:47:56.086[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m420[0m - [1mVerbosity set to DEBUG[0m
[32m2024-12-16 10:48:03.059[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-16 10:48:03.523[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-16 10:48:03.785[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-16 10:48:03.816[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-16 10:48:03.817[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-16 10:48:03.821[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2024-12-16 10:48:03.825[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m452[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2024-12-16 10:48:04.275[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-16 10:48:04.276[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-16 10:48:04.281[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2024-12-16 10:48:04.526[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-16 10:48:04.527[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-16 10:48:04.531[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
[32m2024-12-16 10:48:04.571[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m503[0m - [1mEvaluation tracker args: {'output_path': './logs/'}[0m
[32m2024-12-16 10:48:04.572[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m592[0m - [1mSelected Tasks: ['vqav2_val_lite'][0m
[32m2024-12-16 10:48:04.577[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
initialize llava model with modification
initialize llava model with modification
initialize llava model with modification
OpenCLIP not installed
Failed to import llava_qwen from llava.language_model.llava_qwen. Error: No module named 'llava.model.language_model.llava_qwen'
Failed to import llava_mistral from llava.language_model.llava_mistral. Error: No module named 'llava.model.language_model.llava_mistral'
Failed to import llava_mixtral from llava.language_model.llava_mixtral. Error: No module named 'llava.model.language_model.llava_mixtral'
initialize llava model with modification
OpenCLIP not installed
Failed to import llava_qwen from llava.language_model.llava_qwen. Error: No module named 'llava.model.language_model.llava_qwen'
Failed to import llava_mistral from llava.language_model.llava_mistral. Error: No module named 'llava.model.language_model.llava_mistral'
Failed to import llava_mixtral from llava.language_model.llava_mixtral. Error: No module named 'llava.model.language_model.llava_mixtral'
OpenCLIP not installed
Failed to import llava_qwen from llava.language_model.llava_qwen. Error: No module named 'llava.model.language_model.llava_qwen'
Failed to import llava_mistral from llava.language_model.llava_mistral. Error: No module named 'llava.model.language_model.llava_mistral'
Failed to import llava_mixtral from llava.language_model.llava_mixtral. Error: No module named 'llava.model.language_model.llava_mixtral'
OpenCLIP not installed
Failed to import llava_qwen from llava.language_model.llava_qwen. Error: No module named 'llava.model.language_model.llava_qwen'
Failed to import llava_mistral from llava.language_model.llava_mistral. Error: No module named 'llava.model.language_model.llava_mistral'
Failed to import llava_mixtral from llava.language_model.llava_mixtral. Error: No module named 'llava.model.language_model.llava_mixtral'
self.merging= None
model_name: llava-v1.6-vicuna-7b
Rank 0:  Loaded LLaVA model: liuhaotian/llava-v1.6-vicuna-7b
loding from here
self.merging= None
model_name: llava-v1.6-vicuna-7b
loding from here
self.merging= None
model_name: llava-v1.6-vicuna-7b
loding from here
self.merging= None
model_name: llava-v1.6-vicuna-7b
loding from here
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Rank 0:  Loading vision tower: openai/clip-vit-large-patch14-336
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:10,  5.37s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.58s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.93s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:07<00:14,  7.29s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:05,  5.40s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:10<00:05,  5.39s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:11<00:05,  5.63s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:13<00:06,  6.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.23s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:15<00:00,  5.29s/it]
device: cuda:0
generation_type: recursion
fix_grid: 2x2
attention_thresholding_type: layer_mean_topk
attention_norm: None
attention_threshold: 0.7
detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [1, 0, -1, -2]
positional_embedding_type: bilinear_interpolationdevice: cuda:0

visualize_heatmap: Falsegeneration_type: recursion

square: 1fix_grid: 2x2

attention_thresholding_type: layer_mean_topkremove unpadding=True, change to 'spatial'

attention_norm: None
change positional embedding to bilinear_interpolationattention_threshold: 0.7

detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [1, 0, -1, -2]
positional_embedding_type: bilinear_interpolation
visualize_heatmap: False
square: 1
remove unpadding=True, change to 'spatial'
change positional embedding to bilinear_interpolation
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:16<00:00,  5.56s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:16<00:00,  5.61s/it]
device: cuda:0
generation_type: recursion
fix_grid: 2x2
attention_thresholding_type: layer_mean_topk
attention_norm: None
attention_threshold: 0.7
detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [1, 0, -1, -2]
positional_embedding_type: bilinear_interpolation
visualize_heatmap: False
square: 1
remove unpadding=True, change to 'spatial'
change positional embedding to bilinear_interpolation
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.10s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.31s/it]
Rank 0:  Model Class: LlavaLlamaForCausalLM
device: cuda:0
generation_type: recursion
fix_grid: 2x2
attention_thresholding_type: layer_mean_topk
attention_norm: None
attention_threshold: 0.7
detection_strategy: None
detection_threshold: 0.8
save_output: False
save_output_csv_path: generation_output.csv
target_token_selection_strategy: first
stages: [1, 0, -1, -2]
positional_embedding_type: bilinear_interpolation
visualize_heatmap: False
square: 1
remove unpadding=True, change to 'spatial'
change positional embedding to bilinear_interpolation
Bilienar interpolation embedding type.
[32m2024-12-16 10:48:30.153[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-16 10:48:30.154[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank2-world_size4-tokenizer is not cached, generating...[0m
[32m2024-12-16 10:48:30.155[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 2...[0m
Bilienar interpolation embedding type.
Bilienar interpolation embedding type.
[32m2024-12-16 10:48:30.210[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-16 10:48:30.211[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank3-world_size4-tokenizer is not cached, generating...[0m
[32m2024-12-16 10:48:30.212[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 3...[0m
[32m2024-12-16 10:48:30.216[0m | [1mINFO    [0m | [36mlmms_eval.models.llava[0m:[36m__init__[0m:[36m306[0m - [1mUsing 4 devices with data parallelism[0m
[32m2024-12-16 10:48:30.217[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-16 10:48:30.217[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank0-world_size4-tokenizer is not cached, generating...[0m
[32m2024-12-16 10:48:30.218[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 0...[0m
[32m2024-12-16 10:48:31.263[0m | [1mINFO    [0m | [36mlmms_eval.evaluator_utils[0m:[36mfrom_taskdict[0m:[36m91[0m - [1mNo metadata found in task config for vqav2_val_lite, using default n_shot=0[0m
[32m2024-12-16 10:48:31.264[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m33[0m - [34m[1mrequests-vqav2_val_lite-0shot-rank1-world_size4-tokenizer is not cached, generating...[0m
[32m2024-12-16 10:48:31.266[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m425[0m - [1mBuilding contexts for vqav2_val_lite on rank 1...[0m
  0%|          | 0/125 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 107128.73it/s]
[32m2024-12-16 10:48:31.356[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 125[0m
  0%|          | 0/125 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 100342.20it/s]
[32m2024-12-16 10:48:31.376[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 125[0m
  0%|          | 0/125 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 85514.27it/s]
[32m2024-12-16 10:48:31.389[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 125[0m
  0%|          | 0/125 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:00<00:00, 113359.57it/s]
[32m2024-12-16 10:48:32.416[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m455[0m - [34m[1mTask: vqav2_val_lite; number of requests on this rank: 125[0m
[32m2024-12-16 10:48:40.768[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
[32m2024-12-16 10:48:40.768[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
[32m2024-12-16 10:48:40.768[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
[32m2024-12-16 10:48:40.769[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m476[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/125 [00:00<?, ?it/s]CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
Model Responding:   1%|          | 1/125 [00:09<20:37,  9.98s/it]Model Responding:   2%|‚ñè         | 2/125 [00:18<18:55,  9.23s/it]Model Responding:   2%|‚ñè         | 3/125 [00:27<18:25,  9.06s/it]Model Responding:   3%|‚ñé         | 4/125 [00:35<17:31,  8.69s/it]Model Responding:   4%|‚ñç         | 5/125 [00:45<17:53,  8.95s/it]Model Responding:   5%|‚ñç         | 6/125 [00:53<17:31,  8.84s/it]Model Responding:   6%|‚ñå         | 7/125 [01:02<17:35,  8.95s/it]Model Responding:   6%|‚ñã         | 8/125 [01:11<17:22,  8.91s/it]Model Responding:   7%|‚ñã         | 9/125 [01:21<17:47,  9.20s/it]Model Responding:   8%|‚ñä         | 10/125 [01:31<17:55,  9.35s/it]Model Responding:   9%|‚ñâ         | 11/125 [01:40<17:47,  9.37s/it]Model Responding:  10%|‚ñâ         | 12/125 [01:50<17:41,  9.39s/it]Model Responding:  10%|‚ñà         | 13/125 [01:58<17:08,  9.19s/it]Model Responding:  11%|‚ñà         | 14/125 [02:07<16:48,  9.09s/it]Model Responding:  12%|‚ñà‚ñè        | 15/125 [02:17<17:07,  9.34s/it]Model Responding:  13%|‚ñà‚ñé        | 16/125 [02:26<16:31,  9.09s/it]Model Responding:  14%|‚ñà‚ñé        | 17/125 [02:35<16:15,  9.04s/it]Model Responding:  14%|‚ñà‚ñç        | 18/125 [02:44<16:10,  9.07s/it]Model Responding:  15%|‚ñà‚ñå        | 19/125 [02:52<15:42,  8.89s/it]Model Responding:  16%|‚ñà‚ñå        | 20/125 [03:01<15:36,  8.92s/it]Model Responding:  17%|‚ñà‚ñã        | 21/125 [03:10<15:32,  8.96s/it]Model Responding:  18%|‚ñà‚ñä        | 22/125 [03:19<15:23,  8.96s/it]Model Responding:  18%|‚ñà‚ñä        | 23/125 [03:29<15:30,  9.12s/it]Model Responding:  19%|‚ñà‚ñâ        | 24/125 [03:38<15:14,  9.06s/it]Model Responding:  20%|‚ñà‚ñà        | 25/125 [03:46<14:50,  8.91s/it]Model Responding:  21%|‚ñà‚ñà        | 26/125 [03:55<14:42,  8.91s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 27/125 [04:04<14:24,  8.82s/it]Model Responding:  22%|‚ñà‚ñà‚ñè       | 28/125 [04:13<14:28,  8.95s/it]Model Responding:  23%|‚ñà‚ñà‚ñé       | 29/125 [04:22<14:31,  9.08s/it]Model Responding:  24%|‚ñà‚ñà‚ñç       | 30/125 [04:32<14:40,  9.27s/it]